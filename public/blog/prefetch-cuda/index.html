<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Prefetching Memory in CUDA | Aadi</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Prefetching Memory in CUDA" />
  <meta name="twitter:description" content=""/>
  <meta name="twitter:site" content="@__aadiDev__" />
  <meta name="twitter:creator" content="https://twitter.com/__aadiDev__" />
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />


  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.e698f6b3b067916a386cc08bdd04238ac76cc99155c3a0385cb5b85ea38beb30.css" integrity="sha256-5pj2s7BnkWo4bMCL3QQjisdsyZFVw6A4XLW4XqOL6zA="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/aadi-blogs.web.app\/"
      },
      "articleSection" : "blog",
      "name" : "Prefetching Memory in CUDA",
      "headline" : "Prefetching Memory in CUDA",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "0001",
      "datePublished": "0001-01-01 00:00:00 \u002b0000 UTC",
      "dateModified" : "0001-01-01 00:00:00 \u002b0000 UTC",
      "url" : "https:\/\/aadi-blogs.web.app\/blog\/prefetch-cuda\/",
      "wordCount" : "974",
      "keywords" : ["Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">blog</a>
      </li>
    
      <li>
        <a  href="/code">code</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Prefetching Memory in CUDA</h1>
            
          </header>
          <article class="post__content">
              

<h2 id="threads-blocks-and-grids">Threads, Blocks and Grids<a class="anchor" href="#threads-blocks-and-grids">#</a></h2>
<p>A thread is a single &ldquo;process&rdquo; on GPU. Any given GPU kernel can use <em>blocks</em> of threads, grouped into a <em>grid</em> of blocks. A kernel is executed as a grid of blocks of threads. Each block is run by a single <strong>Streaming Multiprocessor</strong> (SM) and in most usual, single-node cases can&rsquo;t be migrated to other SMs. One SM may execute several CUDA blocks concurrently.</p>
<h2 id="paging">Paging<a class="anchor" href="#paging">#</a></h2>
<p>Paging is a memory-management technique which allows a process&rsquo;s physical address space to be non-contiguous. Paging prevents two main problems:</p>
<ul>
<li>External memory fragmentation and</li>
<li>the associate need for contraction</li>
</ul>
<p>Paging is usually accomplished by breaking physical memory into fixed-sized blocks called <em>frames</em>, and breaking logical memory into blocks of the same size called <strong>pages</strong>. When a process is run, its pages are loaded from secondary memory (file system or backing store) into the memory page. The most interesting aspect of this is that it allows a process to have a logical 64-bit address space, although the system has less than $2^{64}$ bytes of physical memory.</p>
<h2 id="page-faults">Page Faults<a class="anchor" href="#page-faults">#</a></h2>
<p>A page <em>fault</em> occurs when a process requests tries to access a page that wasn&rsquo;t brought into memory (whether it be device or host). The paging hardward will notice that an invalid bit is set, and goes on to execute a straightforward procedure to handle this:</p>
<ol>
<li>Check an internal table for the process to determine whether the reference itself was valid</li>
<li>If the reference was invalid, terminate the process, else we page in the data</li>
<li>Find a free frame in physical memory (a frame is a fixed-size collection of blocks that is indexed in physical memory)</li>
<li>Schedule a secondary storage operation to load the needed page into the newly allocated frame</li>
<li>When the read is complete, the internal table kept with the process and the page table is modified to indicate that the page is now in memory</li>
<li>The instruction is restarted</li>
</ol>
<p>There are two main advantages of GPU page-faulting:</p>
<ul>
<li>the CUDA system doesn&rsquo;t need to sync all managed memory allocations to GPU before each kernel since faulting causes automatic migration</li>
<li>page mapped to GPU addess space</li>
</ul>
<p>The above process takes non-zero time, and a series of page-faults can result in significant memory overhead for any CUDA kernel.</p>
<h2 id="unified-memory">Unified Memory<a class="anchor" href="#unified-memory">#</a></h2>
<p>The CUDA programming model streamlines kernel development by implementing Unified Memory (UM) access, eliminating the need for explicit data movement via <code>cudaMemcp*()</code>. This is since the UM model enables all processes to see a coherent memory image with a common address address, where explicit calls to memory movement is handled by CUDA.</p>
<p>UM is for writing streamlined code, and does not necessarily result in a speed increase.</p>
<p>More significantly, non-explicit allocation of memory resources may result in a large amout of page-faulting procedures which go on during the kernel execution. However, non-explicit allocation of memory resources may result in a large amout of page-faulting procedures which go on during the kernel execution.</p>
<p>According to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning" 
  
   target="_blank" rel="noreferrer noopener" 
>CUDA Performance tuning guidelines</a>:</p>
<ul>
<li>Faults should be avoided: fault handling takes a while since it may include TLB invalidates, data migrations and page table updates</li>
<li>Data should be local to the access processor to minimize memory access latency and maximize bandwidth</li>
<li>Overhead of migration may exceed the benefits of locality if data is constantly migrated</li>
</ul>
<p>Hence we can <em>not</em> use UM, since the UM drives can&rsquo;t detect common access patterns and optimize around it. WHen access patterns are non-obvious, it needs some guidance</p>
<h2 id="prefetching">Prefetching<a class="anchor" href="#prefetching">#</a></h2>
<p>Data prefetching is moving data to a processor&rsquo;s main memory and creating the mapping the page tables BEFORE data processing begins with the aim to avoid faults and establish locality.</p>
<p><code>cudaMemPrefetchhAsync</code> prefetches memory to the specified destination device</p>
<pre><code class="language-cpp">    cudaError_t cudaMemPrefetchAsync(
        const void *devPtr,     // memory region
        size_t count,           // number of bytes
        inst dstDevice,         // device ID
        cudaStream_t stream
    );
</code></pre>
<h2 id="profiling-prefetches">Profiling Prefetches<a class="anchor" href="#profiling-prefetches">#</a></h2>
<p>The following shows profile statistics using <code>nsys</code> of two kernel which do exactly the same operation (squaring a series of float values on GPU).</p>
<h3 id="with-prefetching">With prefetching<a class="anchor" href="#with-prefetching">#</a></h3>
<pre><code>
CUDA Memory Operation Statistics (by time):

 Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)              Operation
 --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ---------------------------------
    100.0       10,592,835     64  165,513.0  165,313.0   165,249   178,465      1,645.6  [CUDA Unified Memory memcpy DtoH]


CUDA Memory Operation Statistics (by size):

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation
 ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------
    134.218     64     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy DtoH]

</code></pre>
<h3 id="without-prefetching">Without prefetching<a class="anchor" href="#without-prefetching">#</a></h3>
<pre><code>CUDA Memory Operation Statistics (by time):

 Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation
 --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------
     67.0       22,186,252  1,536  14,444.2   3,935.0     1,278   120,226     23,567.2  [CUDA Unified Memory memcpy HtoD]
     33.0       10,937,857    768  14,242.0   3,359.0       895   102,529     23,680.5  [CUDA Unified Memory memcpy DtoH]

[9/9] Executing 'gpumemsizesum' stats report

CUDA Memory Operation Statistics (by size):

 Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation
 ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------
    268.435  1,536     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]
    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]
</code></pre>
<!-- `void* malloc (size_t size);` allocates a block of `size` memory, returning a pointer to the beginning of this block. `cudaMalloc()` does the same for linear memory, typically copy from host to device using `cudaMemcpy()`.
 -->
<p>Of note are two things: the time taken for memory operations without prefetching is nearly triple that of the prefetch, and the size of memory operations is much smaller. The only difference between both kernels is a call to <code>cudaMemPrefetchAsync</code> for any data structure that was to be copied to device (GPU).</p>
<pre><code class="language-cpp">int deviceId;

const int N = 2&lt;&lt;24;
size_t size = N * sizeof(float);
// Declare a float pointer
float *a;
// Set up unified memory
cudaMallocManaged(&amp;a, size);

// Up till this point is usual, the only difference is this call to the prefetch
cudaMemPrefetchAsync(a, size, deviceId);

// Go on to specify the number of threads, blocks etc.
</code></pre>


              
                  

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>
              
          </article>
          

 <div class="pagination">
  

  
    <a class="pagination__item" href="https://aadi-blogs.web.app/blog/graph-neural-network/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >The Graph Neural Network</span>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a
        class="social-icons__link"
        title="Twitter"
        href="https://twitter.com/__aadiDev__"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/twitter.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="GitHub"
        href="https://github.com/aadi350"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/github.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="LinkedIn"
        href="https://www.linkedin.com/in/aadidev-sooknanan/"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/linkedin.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="Email"
        href="mailto:aadidevsooknanan@gmail.com"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/email.svg')"></div>
      </a>
    
     
</div>

            <p>©2022 2022</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js" integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
