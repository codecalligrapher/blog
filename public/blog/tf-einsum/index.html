<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Unravelling `tf.einsum` | Aadi</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Unravelling `tf.einsum`" />
  <meta name="twitter:description" content=""/>
  <meta name="twitter:site" content="@__aadiDev__" />
  <meta name="twitter:creator" content="https://twitter.com/__aadiDev__" />
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />


  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.e698f6b3b067916a386cc08bdd04238ac76cc99155c3a0385cb5b85ea38beb30.css" integrity="sha256-5pj2s7BnkWo4bMCL3QQjisdsyZFVw6A4XLW4XqOL6zA="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/aadi-blogs.web.app\/"
      },
      "articleSection" : "blog",
      "name" : "Unravelling `tf.einsum`",
      "headline" : "Unravelling `tf.einsum`",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2022",
      "datePublished": "2022-07-18 23:00:42 -0400 AST",
      "dateModified" : "2022-07-18 23:00:42 -0400 AST",
      "url" : "https:\/\/aadi-blogs.web.app\/blog\/tf-einsum\/",
      "wordCount" : "934",
      "keywords" : ["Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">blog</a>
      </li>
    
      <li>
        <a  href="/code">code</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Unravelling `tf.einsum`</h1>
            <time datetime="2022-07-18 23:00:42 -0400 AST" class="post__date">Jul 18 2022</time> 
          </header>
          <article class="post__content">
              

<h2 id="origin-story">Origin Story<a class="anchor" href="#origin-story">#</a></h2>
<p>Recently, I was trying to disect the original <a href="https://arxiv.org/abs/1511.02136v6" 
  
   target="_blank" rel="noreferrer noopener" 
>DCNN Paper</a> which utilized a <em>diffusion</em> kernel to more readily make use of implicit graph-structure in common tasks such as node, edge and graph classification. However, an existing implementation I fonund had a curious piece of notation which led me down the rabbithole of Tensor calculus.</p>
<p>Coordinates are maps used to solve a given problem. A coordinate transform allows mapping from one frame of reference to another (converting from a map of your high school, to the location of your high school in reference to where it is in the city, compared to a country-wide map).</p>
<p>To say a number is a sclar means that the value does no change when transformed from one coordinate system to another (e.g. the distance between two points on a flat plain is irrespective of where true north is).</p>
<p>A vector is directional, and can be formed on the basis of the reference set of coordinates. For example, a vector between your home and the nearest fire-station can be broken down into a sum of north- and east-facing vectors.</p>
<h2 id="tensors">Tensors<a class="anchor" href="#tensors">#</a></h2>
<p>A tensor describes the superset of transformations which include scalars and vectors:</p>
<ul>
<li>$0$-tensors are constant functions, which we identify as scalars</li>
<li>$1$-tensors are linear functions, which we call vectors</li>
<li>$2$-tensors are bilinear functions, which we call matrices</li>
</ul>
<p>A <strong>Tensor</strong> describes any general transformation, independent of any basis function between sets of algebraic objects related to a vector space</p>
<hr>
<p>Back to the paper, there was a particular function which claimed to do batch matrix multiplication:</p>
<pre><code class="language-python">tf.einsum('ijk,kl-&gt;ijl', A, B)
</code></pre>
<p>where $A$ was the diffusion kernel and $B$ was a feature vector (and <code>tf</code> was <code>tensorflow</code>). So $A$ would have dimensions (batch_size, m, n) and $B$ would have dimensions (n, k), where:</p>
<ul>
<li>batch_size: number of nodes to process in a given batch (for model trainining)</li>
<li>n: number of features</li>
<li>m: number of nodes</li>
<li>k: number of &ldquo;hops&rdquo;</li>
</ul>
<p>Ignoring the technicalities of the paper and the actual definitions above, I wanted to know what the actual heck this strange <code>einsum</code> function was trying to do</p>
<h2 id="einstein-summation">Einstein Summation<a class="anchor" href="#einstein-summation">#</a></h2>
<p>Enter <em>Einstein</em> summation:
In &ldquo;Einstein&rdquo; summation, the repeated index defines what we sum by, the expression must have a repeated index, so:
$$
\sum_{i=1}^n = a_1x_1 + a_2x_2 + &hellip; + a_nx_n \equiv a_ix_i
$$
is valid. But $a_{ij}x_k$ is not, whilst $a_{ij}x_j$ is:
$$
a_{ij}x_j \equiv a_{i1}x_1 + a_{i2}x_2 + &hellip; + a_{in}x_n
$$</p>
<p>Double sums are handled as follows, for example summation on both $i$ and $j$:
$$
a_{ij}x_iy_j
$$</p>
<p>In the <code>einsum</code> function, the first argument <code>ijk,kl-&gt;ijl</code> signified summation on the $k^{th}$ dimension</p>
<hr>
<p>Now that I understood what the notation meant, I wanted a better grasp on the actual mechanics behind the function. Using synthetic Tensors as follows:</p>
<pre><code class="language-python">k = 2
batch_size, m, n = None, 4, 2
init = tf.random.uniform(shape=(m, n), minval=0, maxval=16, dtype=tf.int32)
A = tf.Variable(init)
A = tf.expand_dims(A, 0)
A
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=
array([[[14,  4],
        [ 4, 12],
        [ 9, 13],
        [ 0, 13]]], dtype=int32)&gt;
</code></pre>
<pre><code class="language-python">init = tf.random.uniform(shape=(n, k), minval=0, maxval=16, dtype=tf.int32)
B = tf.Variable(init)
B
</code></pre>
<pre><code>&lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=int32, numpy=
array([[3, 9],
       [5, 1]], dtype=int32)&gt;
</code></pre>
<h3 id="tfmatmul"><code>tf.matmul</code><a class="anchor" href="#tfmatmul">#</a></h3>
<p>Here is where I used the two prior defined Tensors to basically see what would happen. It was also at this point I realised that TensorFlow 2 now included a function <code>matmul</code> which essentially did the same!</p>
<pre><code class="language-python">C = tf.einsum('ijk,kl-&gt;ijl', A, B)
C
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=
array([[[ 62, 130],
        [ 72,  48],
        [ 92,  94],
        [ 65,  13]]], dtype=int32)&gt;
</code></pre>
<pre><code class="language-python">tf.matmul(A, B)
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(1, 4, 2), dtype=int32, numpy=
array([[[ 62, 130],
        [ 72,  48],
        [ 92,  94],
        [ 65,  13]]], dtype=int32)&gt;
</code></pre>
<h2 id="minimum-viable-example">Minimum-Viable Example<a class="anchor" href="#minimum-viable-example">#</a></h2>
<p>Okay, now simplifying even further; firstly by creating a rank-2 tensor (i.e. a matrix) using numpy and then finding the matrix product</p>
<pre><code class="language-python">import numpy as np

A = np.matrix('''
    1 4;
    2 3
''')

B = np.matrix('''
    5 7;
    6 8
''')

C = A @ B
C
</code></pre>
<pre><code>matrix([[29, 39],
        [28, 38]])
</code></pre>
<p>Every element in $C$, $C_{ik}$ is:
$$
C_{ik} = \sum_jA_{ij}B_{jk}
$$</p>
<p>$C_{01} = 39$ so</p>
<p>$$
C_{01} = \sum_j A_{0j} B_{j1} (1\times 7)_ {j=0} + (4\times 8)_{j=1}
$$</p>
<p>Followed by converting the above matrices to TensorFlow objects and repeating the operation to somehow test that I grasped the notation:</p>
<pre><code class="language-python">A = tf.convert_to_tensor(A)
B = tf.convert_to_tensor(B)

A, B
</code></pre>
<pre><code>(&lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy=
 array([[1, 4],
        [2, 3]])&gt;,
 &lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy=
 array([[5, 7],
        [6, 8]])&gt;)
</code></pre>
<p>It worked! The output of <code>einsum</code> below is consistent with <code>matmul</code> above</p>
<pre><code class="language-python"># equivalent to A @ B or tf.matmul(A, B)
tf.einsum('ij,jk-&gt;ik', A, B)
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[29, 39],
       [28, 38]])&gt;
</code></pre>
<h2 id="slightly-less-minimum-example">Slightly-Less Minimum Example<a class="anchor" href="#slightly-less-minimum-example">#</a></h2>
<p>Now on to a slightly more complex example, I created a rank-2 Tensor and a rank-1 Tensor for multiplication against</p>
<pre><code class="language-python"># applying to batch case
A = tf.Variable([
    [[1,2],
    [3,4]],
    [[3, 5], 
    [2, 9]]
])

B = tf.Variable(
    [[2], [1]]
)
A.shape, B.shape
</code></pre>
<pre><code>(TensorShape([2, 2, 2]), TensorShape([2, 1]))
</code></pre>
<pre><code class="language-python">tf.matmul(A, B)
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=
array([[[ 4],
        [10]],

       [[11],
        [13]]], dtype=int32)&gt;
</code></pre>
<p>For the $ijl^{th}$ element in $C$, sum across the $k^{th}$ dimension in A and B</p>
<pre><code>output[i,j,l] = sum_k A[i,j,k] * B[k, l]
</code></pre>
<pre><code class="language-python"># for the ijl-th element in C, 
C = tf.einsum('ijk,kl-&gt;ijl', A, B)
C
</code></pre>
<pre><code>&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=
array([[[ 4],
        [10]],

       [[11],
        [13]]], dtype=int32)&gt;
</code></pre>
<p>and success! I think I have a fair grasp on how <code>einsum</code> and Einstein summation works, and how/why it can be sometimes simpler just to use the built-in <code>matmul</code> function, but also where batch dimensions may mess with the built-in functions and we would want to define it in finer detail</p>


              
                  

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>
              
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="https://aadi-blogs.web.app/blog/matrix-graph/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">Basics of The Adjacency Matrix</span>
    </a>
  

  
    <a class="pagination__item" href="https://aadi-blogs.web.app/blog/graph-diffusion/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Graph Diffusion</span>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a
        class="social-icons__link"
        title="Twitter"
        href="https://twitter.com/__aadiDev__"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/twitter.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="GitHub"
        href="https://github.com/aadi350"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/github.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="LinkedIn"
        href="https://www.linkedin.com/in/aadidev-sooknanan/"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/linkedin.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="Email"
        href="mailto:aadidevsooknanan@gmail.com"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/email.svg')"></div>
      </a>
    
     
</div>

            <p>©2022 2022</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js" integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
