<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Graph Diffusion | Aadis Blog</title>
<meta name="keywords" content="">
<meta name="description" content="This is taken from Diffusion Convolutional Neural Networks (referenced in the footer). According to the authors, a diffusion convolution scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.
It was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i.">
<meta name="author" content="">
<link rel="canonical" href="https://aadi-blogs.web.app/blog/graph-diffusion/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css" integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://aadi-blogs.web.app/favicon.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://aadi-blogs.web.app/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://aadi-blogs.web.app/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://aadi-blogs.web.app/apple-touch-icon.png">
<link rel="mask-icon" href="https://aadi-blogs.web.app/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Graph Diffusion" />
<meta property="og:description" content="This is taken from Diffusion Convolutional Neural Networks (referenced in the footer). According to the authors, a diffusion convolution scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.
It was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aadi-blogs.web.app/blog/graph-diffusion/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2022-07-25T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2022-07-25T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Graph Diffusion"/>
<meta name="twitter:description" content="This is taken from Diffusion Convolutional Neural Networks (referenced in the footer). According to the authors, a diffusion convolution scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.
It was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i."/>
<meta name="twitter:site" content="@https://twitter.com/cats"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Blogs",
      "item": "https://aadi-blogs.web.app/blog/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Graph Diffusion",
      "item": "https://aadi-blogs.web.app/blog/graph-diffusion/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Graph Diffusion",
  "name": "Graph Diffusion",
  "description": "This is taken from Diffusion Convolutional Neural Networks (referenced in the footer). According to the authors, a diffusion convolution scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.\nIt was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i.",
  "keywords": [
    
  ],
  "articleBody": " This is taken from Diffusion Convolutional Neural Networks (referenced in the footer). According to the authors, a diffusion convolution scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.\nIt was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i.e. graphs). Based on the results presented, the DCNN model outperformed a probabilistic-relational model in citation (a conditional-random field, no I do not know much about that) in graph topic-classification.\nDiffusion outperforms probabilistic relational methods, is flexible to handle graphs with node features, edge features and purely structural information, is polynomial-time, model only has $H\\times F$ parameters (H is num hops, F is num features), completed with a dense layer connecting $Z$ to the conditional prediction\nMain point in the paper was using a novel way to propagate the features throughout the graph structure\nimport networkx as nx import tensorflow as tf import numpy as np Simple Example This is getting more intimate with Graph Diffusion, since the term seemed a bit hand-wavy (especially as the authors define it by quoting as a “diffusion mechanism”):\n“Briefly, rather than scanning a ‘square’ of parameters across a grid-structured input like the standard convolution operation, the diffusion-convolution operation builds a latent representation by scanning a diffusion process across each node in a graph-structured input”\n# define a simple adjacency matrix A = np.matrix(''' 0 1 0 1 0; 1 0 0 1 1; 0 0 0 1 0; 1 1 1 0 0; 0 1 0 0 0 ''') # feature matrix X = np.matrix(''' 2 0; 0 4; 3 1; 5 6; 9 3 ''') # labels Y = np.matrix(''' 0 1; 1 0; 1 0; 1 0; 0 1 ''') print(f'{A.shape=}, {X.shape=}, {Y.shape=}') # just for plotting G = nx.from_numpy_matrix(A) nx.draw_networkx(G, with_labels=True) A.shape=(5, 5), X.shape=(5, 2), Y.shape=(5, 2) The diffusion kernel is given by: $$ [A^{0}, A^{1},…,A^{k-1}] $$\nwhere $A$ is the adjacency matrix, and $k$ is the number of 1-hops. Normalizing the adjacency matrix is achived by dividing by the degree matrix $D$ at each stage, so it ends up being:\n$$ A^{i+1} = \\frac{A}{d}\\times A^{i} $$\nExperimenting with $k=2$ hops, so the first dimension of $K$ would be $3$ if self-loops are included.\nk = 2 # num-hops K = [np.identity(A.shape[0])] d = A.sum(0) K.append(A / (d+1.0)) for i in range(2, k+1): K.append( np.dot(A/(d+1.0), K[-1]) ) K = np.array(K, dtype=np.float32) # at this point, shape is [k+1, num_nodes, num_nodes] K = np.transpose(K, (1, 0, 2)) # here it becomes (num_nodes, k+1, num_nodes) K.shape (5, 3, 5) Indexes diffusion kernel for node one, here’s how to read it:\n$i^{th}$ index is number of hops\n$j^{th}$ index is to which node\nSo the probability of jumping to node 1 in 1 hop 0.25 (zero-indexed), whilst probabilities for all nodes in $0$ hops are $0$ for obvious reasons\nK[0] array([[1. , 0. , 0. , 0. , 0. ], [0. , 0.25 , 0. , 0.25 , 0. ], [0.16666667, 0.0625 , 0.125 , 0.0625 , 0.125 ]], dtype=float32) Once the diffusion kernel is defined, the next step was to weight the product of the diffusion kernel with node features using a weight matrix:\n$$ Z = f(W^c\\odot P^*X) $$\n$W$ is a weight matrix, let $P^*X$ be $PX$\nThe product of the following is indexed as follows:\n$i^{th}$ indexes an individual node $j^{th}$ is number of hops $k^{th}$ is feature (I think) PX captures the probability of hopping, hence allows the features to propagate conditional to their $k$-hop distance to neighbours\nK, X # K is kernel, X is features (array([[[1. , 0. , 0. , 0. , 0. ], [0. , 0.25 , 0. , 0.25 , 0. ], [0.16666667, 0.0625 , 0.125 , 0.0625 , 0.125 ]], [[0. , 1. , 0. , 0. , 0. ], [0.33333334, 0. , 0. , 0.25 , 0.5 ], [0.08333334, 0.27083334, 0.125 , 0.08333334, 0. ]], [[0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0. , 0.25 , 0. ], [0.08333334, 0.0625 , 0.125 , 0. , 0. ]], [[0. , 0. , 0. , 1. , 0. ], [0.33333334, 0.25 , 0.5 , 0. , 0. ], [0.08333334, 0.08333334, 0. , 0.27083334, 0.125 ]], [[0. , 0. , 0. , 0. , 1. ], [0. , 0.25 , 0. , 0. , 0. ], [0.08333334, 0. , 0. , 0.0625 , 0.125 ]]], dtype=float32), matrix([[2, 0], [0, 4], [3, 1], [5, 6], [9, 3]])) PX = np.einsum('ijk,kl-\u003eijl', K, X) # same as tf.matmul(K, X) PX, PX.shape (array([[[2. , 0. ], [1.25 , 2.5 ], [2.14583334, 1.125 ]], [[0. , 4. ], [6.41666669, 3. ], [0.95833335, 1.70833339]], [[3. , 1. ], [1.25 , 1.5 ], [0.54166667, 0.375 ]], [[5. , 6. ], [2.16666669, 1.5 ], [2.64583339, 2.3333334 ]], [[9. , 3. ], [0. , 1. ], [1.60416667, 0.75 ]]]), (5, 3, 2)) PX[1] array([[0. , 4. ], [6.41666669, 3. ], [0.95833335, 1.70833339]]) $K$ =\n[[[1. , 0. , 0. , 0. , 0. ], Hop 1 -\u003e [0. , 0.25 , 0. , 0.25 , 0. ], [0.16666667, 0.0625 , 0.125 , 0.0625 , 0.125 ]], [[0. , 1. , 0. , 0. , 0. ], [0.33333334, 0. , 0. , 0.25 , 0.5 ], [0.08333334, 0.27083334, 0.125 , 0.08333334, 0. ]], [[0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0. , 0.25 , 0. ], [0.08333334, 0.0625 , 0.125 , 0. , 0. ]], [[0. , 0. , 0. , 1. , 0. ], [0.33333334, 0.25 , 0.5 , 0. , 0. ], [0.08333334, 0.08333334, 0. , 0.27083334, 0.125 ]], [[0. , 0. , 0. , 0. , 1. ], [0. , 0.25 , 0. , 0. , 0. ], [0.08333334, 0. , 0. , 0.0625 , 0.125 ]]] $X$ =\n[[2, 0], [0, 4], [3, 1], [5, 6], [9, 3]] For node $0$, hop $1$, feature $1$, we take $K[0, 1]$ times $X[0, 1]$:\nThis is less-optimized, but more index-friendly of rewriting the equation prior $$ Z_ {ijk} = f\\left(W^{c}_ {jk}\\cdot\\sum_{l=1}^{N} P_{ijl}X_{lk}\\right) $$\n$N$ is number of nodes\ntrying $(i=1, j=1, k=0)$, which should result in $PX[1, 1, 0]$\nK[1, 1, :], X[:, 0] (array([0.33333334, 0. , 0. , 0.25 , 0.5 ], dtype=float32), matrix([[0], [4], [1], [6], [3]])) K[0, 1, 1] * X[:, 1] matrix([[0. ], [1. ], [0.25], [1.5 ], [0.75]]) np.sum(K[1, 1, :] * X[:, 0]) # It works! It's equal to PX[1, 1 ,0] 6.416666686534882 PX[1,1,0] 6.416666686534882 Past this point, $Z$ is weighted once again using another set of weights and then subject to a nonlinear activation function, standard layers implemented by keras layers and hence less intellectually stimulating at first glance. I may return to this paper from a purely philosophical standpoint, but the previous notes cover the parts of Tensor calculus that confused me the most.\nFor interested readers, I suggest the paper linked\nReferences Atwood, J., \u0026 Towsley, D. (2016). Diffusion-convolutional neural networks. Advances in neural information processing systems, 29\n",
  "wordCount" : "1202",
  "inLanguage": "en",
  "datePublished": "2022-07-25T00:00:00Z",
  "dateModified": "2022-07-25T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://aadi-blogs.web.app/blog/graph-diffusion/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Aadis Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://aadi-blogs.web.app/favicon.png"
    }
  }
}
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>


<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false }
            ]
        });
    });
</script>

</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://aadi-blogs.web.app/" accesskey="h" title="Aadis Blog (Alt + H)">Aadis Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://aadi-blogs.web.app/blog/" title="blogs">
                    <span><i class='fa fa-heart'></i>blogs</span>
                </a>
            </li>
            <li>
                <a href="https://aadi-blogs.web.app/code/" title="code">
                    <span><i class='fa fa-heart'></i>code</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://aadi-blogs.web.app/">Home</a>&nbsp;»&nbsp;<a href="https://aadi-blogs.web.app/blog/">Blogs</a></div>
    <h1 class="post-title">
      Graph Diffusion
    </h1>
    <div class="post-meta"><span title='2022-07-25 00:00:00 +0000 +0000'>July 25, 2022</span>&nbsp;·&nbsp;6 min

</div>
  </header> 
  <div class="post-content">
<p>This is taken from <a href="https://arxiv.org/pdf/1511.02136v6.pdf">Diffusion Convolutional Neural Networks</a> (referenced in the footer). According to the authors, a <strong>diffusion convolution</strong> scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.</p>
<p>It was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i.e. graphs). Based on the results presented, the DCNN model outperformed a probabilistic-relational model in citation (a <em>conditional-random field</em>, no I do not know much about that) in graph topic-classification.</p>
<p>Diffusion outperforms probabilistic relational methods, is flexible to handle graphs with node features, edge features and purely structural information, is polynomial-time, model only has $H\times F$ parameters (H is num hops, F is num features), completed with a dense layer connecting $Z$ to the conditional prediction</p>
<p>Main point in the paper was using a novel way to propagate the features throughout the graph structure</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> networkx <span style="color:#66d9ef">as</span> nx
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span></code></pre></div><h1 id="simple-example">Simple Example<a hidden class="anchor" aria-hidden="true" href="#simple-example">#</a></h1>
<p>This is getting more intimate with Graph Diffusion, since the term seemed a bit hand-wavy (especially as the authors define it by quoting as a &ldquo;diffusion mechanism&rdquo;):</p>
<blockquote>
<p><em>&ldquo;Briefly, rather than scanning a ‘square’ of parameters across a grid-structured input like the standard convolution operation, the diffusion-convolution operation builds a latent representation by scanning a diffusion process across each node in a graph-structured input&rdquo;</em></p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># define a simple adjacency matrix</span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(<span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 1 0 1 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    1 0 0 1 1;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 0 0 1 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    1 1 1 0 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 1 0 0 0
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># feature matrix</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(<span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    2 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 4;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    3 1;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    5 6;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    9 3
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># labels </span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>matrix(<span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 1;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    1 0; 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    1 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    1 0;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    0 1
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>A<span style="color:#f92672">.</span>shape<span style="color:#e6db74">=}</span><span style="color:#e6db74">, </span><span style="color:#e6db74">{</span>X<span style="color:#f92672">.</span>shape<span style="color:#e6db74">=}</span><span style="color:#e6db74">, </span><span style="color:#e6db74">{</span>Y<span style="color:#f92672">.</span>shape<span style="color:#e6db74">=}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># just for plotting</span>
</span></span><span style="display:flex;"><span>G <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>from_numpy_matrix(A)
</span></span><span style="display:flex;"><span>nx<span style="color:#f92672">.</span>draw_networkx(G, with_labels<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><pre><code>A.shape=(5, 5), X.shape=(5, 2), Y.shape=(5, 2)
</code></pre>
<!-- raw HTML omitted -->
<p>The diffusion kernel is given by:
$$
[A^{0}, A^{1},&hellip;,A^{k-1}]
$$</p>
<p>where $A$ is the adjacency matrix, and $k$ is the number of 1-hops. Normalizing the adjacency matrix is achived by dividing by the degree matrix $D$ at each stage, so it ends up being:</p>
<p>$$
A^{i+1} = \frac{A}{d}\times A^{i}
$$</p>
<p>Experimenting with $k=2$ hops, so the first dimension of $K$ would be $3$ if self-loops are included.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>k <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#75715e"># num-hops</span>
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> [np<span style="color:#f92672">.</span>identity(A<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>d <span style="color:#f92672">=</span> A<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K<span style="color:#f92672">.</span>append(A <span style="color:#f92672">/</span> (d<span style="color:#f92672">+</span><span style="color:#ae81ff">1.0</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>, k<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>    K<span style="color:#f92672">.</span>append(
</span></span><span style="display:flex;"><span>        np<span style="color:#f92672">.</span>dot(A<span style="color:#f92672">/</span>(d<span style="color:#f92672">+</span><span style="color:#ae81ff">1.0</span>), K[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(K, dtype<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>float32) <span style="color:#75715e"># at this point, shape is [k+1, num_nodes, num_nodes]</span>
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>transpose(K, (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>)) <span style="color:#75715e"># here it becomes (num_nodes, k+1, num_nodes)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>K<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre><code>(5, 3, 5)
</code></pre>
<p>Indexes diffusion kernel for node one, here&rsquo;s how to read it:</p>
<p>$i^{th}$ index is number of hops<br>
$j^{th}$ index is to which node</p>
<p>So the probability of jumping to node 1 in 1 hop 0.25 (zero-indexed), whilst probabilities for all nodes in $0$ hops are $0$ for obvious reasons</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>K[<span style="color:#ae81ff">0</span>] 
</span></span></code></pre></div><pre><code>array([[1.        , 0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.25      , 0.        , 0.25      , 0.        ],
       [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
      dtype=float32)
</code></pre>
<p>Once the diffusion kernel is defined, the next step was to weight the product of the diffusion kernel with node features using a weight matrix:</p>
<p>$$
Z = f(W^c\odot P^*X)
$$</p>
<p>$W$ is a weight matrix, let $P^*X$ be $PX$<br>
The product of the following is indexed as follows:</p>
<ul>
<li>$i^{th}$ indexes an individual node</li>
<li>$j^{th}$ is number of hops</li>
<li>$k^{th}$ is feature (I think)</li>
</ul>
<p>PX captures the probability of hopping, hence allows the features to propagate conditional to their $k$-hop distance to neighbours</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>K, X <span style="color:#75715e"># K is kernel, X is features</span>
</span></span></code></pre></div><pre><code>(array([[[1.        , 0.        , 0.        , 0.        , 0.        ],
         [0.        , 0.25      , 0.        , 0.25      , 0.        ],
         [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
 
        [[0.        , 1.        , 0.        , 0.        , 0.        ],
         [0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
         [0.08333334, 0.27083334, 0.125     , 0.08333334, 0.        ]],
 
        [[0.        , 0.        , 1.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.25      , 0.        ],
         [0.08333334, 0.0625    , 0.125     , 0.        , 0.        ]],
 
        [[0.        , 0.        , 0.        , 1.        , 0.        ],
         [0.33333334, 0.25      , 0.5       , 0.        , 0.        ],
         [0.08333334, 0.08333334, 0.        , 0.27083334, 0.125     ]],
 
        [[0.        , 0.        , 0.        , 0.        , 1.        ],
         [0.        , 0.25      , 0.        , 0.        , 0.        ],
         [0.08333334, 0.        , 0.        , 0.0625    , 0.125     ]]],
       dtype=float32),
 matrix([[2, 0],
         [0, 4],
         [3, 1],
         [5, 6],
         [9, 3]]))
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>PX <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>einsum(<span style="color:#e6db74">&#39;ijk,kl-&gt;ijl&#39;</span>, K, X) <span style="color:#75715e"># same as tf.matmul(K, X)</span>
</span></span><span style="display:flex;"><span>PX, PX<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><pre><code>(array([[[2.        , 0.        ],
         [1.25      , 2.5       ],
         [2.14583334, 1.125     ]],
 
        [[0.        , 4.        ],
         [6.41666669, 3.        ],
         [0.95833335, 1.70833339]],
 
        [[3.        , 1.        ],
         [1.25      , 1.5       ],
         [0.54166667, 0.375     ]],
 
        [[5.        , 6.        ],
         [2.16666669, 1.5       ],
         [2.64583339, 2.3333334 ]],
 
        [[9.        , 3.        ],
         [0.        , 1.        ],
         [1.60416667, 0.75      ]]]),
 (5, 3, 2))
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>PX[<span style="color:#ae81ff">1</span>]
</span></span></code></pre></div><pre><code>array([[0.        , 4.        ],
       [6.41666669, 3.        ],
       [0.95833335, 1.70833339]])
</code></pre>
<p>$K$ =</p>
<pre tabindex="0"><code>        [[[1.        , 0.        , 0.        , 0.        , 0.        ],
Hop 1 -&gt; [0.        , 0.25      , 0.        , 0.25      , 0.        ],
         [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
 
        [[0.        , 1.        , 0.        , 0.        , 0.        ],
         [0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
         [0.08333334, 0.27083334, 0.125     , 0.08333334, 0.        ]],
 
        [[0.        , 0.        , 1.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.25      , 0.        ],
         [0.08333334, 0.0625    , 0.125     , 0.        , 0.        ]],
 
        [[0.        , 0.        , 0.        , 1.        , 0.        ],
         [0.33333334, 0.25      , 0.5       , 0.        , 0.        ],
         [0.08333334, 0.08333334, 0.        , 0.27083334, 0.125     ]],
 
        [[0.        , 0.        , 0.        , 0.        , 1.        ],
         [0.        , 0.25      , 0.        , 0.        , 0.        ],
         [0.08333334, 0.        , 0.        , 0.0625    , 0.125     ]]]
</code></pre><p>$X$ =</p>
<pre tabindex="0"><code>        [[2, 0],
        [0, 4],
        [3, 1],
        [5, 6],
        [9, 3]]
</code></pre><p>For node $0$, hop $1$, feature $1$, we take $K[0, 1]$ times $X[0, 1]$:</p>
<p>This is less-optimized, but more index-friendly of rewriting the equation prior
$$
Z_ {ijk} = f\left(W^{c}_ {jk}\cdot\sum_{l=1}^{N} P_{ijl}X_{lk}\right)
$$</p>
<p>$N$ is number of nodes</p>
<p>trying $(i=1, j=1, k=0)$, which should result in $PX[1, 1, 0]$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>K[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, :], X[:, <span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><pre><code>(array([0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
       dtype=float32),
 matrix([[0],
         [4],
         [1],
         [6],
         [3]]))
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>K[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> X[:, <span style="color:#ae81ff">1</span>]
</span></span></code></pre></div><pre><code>matrix([[0.  ],
        [1.  ],
        [0.25],
        [1.5 ],
        [0.75]])
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="color:#f92672">.</span>sum(K[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, :] <span style="color:#f92672">*</span> X[:, <span style="color:#ae81ff">0</span>]) <span style="color:#75715e"># It works! It&#39;s equal to PX[1, 1 ,0]</span>
</span></span></code></pre></div><pre><code>6.416666686534882
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>PX[<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><pre><code>6.416666686534882
</code></pre>
<p>Past this point, $Z$ is weighted once again using another set of weights and then subject to a nonlinear activation function, standard layers implemented by <code>keras</code> layers and hence less intellectually stimulating at first glance. I may return to this paper from a purely philosophical standpoint, but the previous notes cover the parts of Tensor calculus that confused me the most.</p>
<p>For interested readers, I suggest the paper linked</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p><a href="https://arxiv.org/pdf/1511.02136v6.pdf">Atwood, J., &amp; Towsley, D. (2016). Diffusion-convolutional neural networks. Advances in neural information processing systems, 29</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on twitter"
        href="https://twitter.com/intent/tweet/?text=Graph%20Diffusion&amp;url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f&amp;title=Graph%20Diffusion&amp;summary=Graph%20Diffusion&amp;source=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f&title=Graph%20Diffusion">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on whatsapp"
        href="https://api.whatsapp.com/send?text=Graph%20Diffusion%20-%20https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Graph Diffusion on telegram"
        href="https://telegram.me/share/url?text=Graph%20Diffusion&amp;url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-diffusion%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://aadi-blogs.web.app/">Aadis Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
