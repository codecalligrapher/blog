<!DOCTYPE html>
<html lang="en-us">

<head>
  <title>Graph Diffusion | Aadi</title>

  <meta charset="UTF-8">
  <meta name="language" content="en">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  
  
  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Graph Diffusion" />
  <meta name="twitter:description" content=""/>
  <meta name="twitter:site" content="@tikamasaala" />
  <meta name="twitter:creator" content="https://twitter.com/tikamasaala" />
  

  <link rel="shortcut icon" type="image/png" href="/favicon.ico" />


  
  
    
 
  
  
  
  
  
  
    
    <link type="text/css" rel="stylesheet" href="/css/post.min.e698f6b3b067916a386cc08bdd04238ac76cc99155c3a0385cb5b85ea38beb30.css" integrity="sha256-5pj2s7BnkWo4bMCL3QQjisdsyZFVw6A4XLW4XqOL6zA="/>
  
    
    <link type="text/css" rel="stylesheet" href="/css/custom.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css" integrity="sha256-47DEQpj8HBSa&#43;/TImW&#43;5JCeuQeRkm5NMpJWZG3hSuFU="/>
  
  
   
   
    

<script type="application/ld+json">
  
    {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/aadi-blogs.web.app\/"
      },
      "articleSection" : "blog",
      "name" : "Graph Diffusion",
      "headline" : "Graph Diffusion",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2022",
      "datePublished": "2022-07-25 00:00:00 \u002b0000 \u002b0000",
      "dateModified" : "2022-07-25 00:00:00 \u002b0000 \u002b0000",
      "url" : "https:\/\/aadi-blogs.web.app\/blog\/graph-diffusion\/",
      "wordCount" : "1202",
      "keywords" : ["Blog"]
    }
  
  </script>
</head>

<body>
  <div class="burger__container">
  <div class="burger" aria-controls="navigation" aria-label="Menu">
    <div class="burger__meat burger__meat--1"></div>
    <div class="burger__meat burger__meat--2"></div>
    <div class="burger__meat burger__meat--3"></div>
  </div>
</div>
 

  <nav class="nav" id="navigation">
  <ul class="nav__list">
    
    
      <li>
        <a  href="/">about</a>
      </li>
    
      <li>
        <a  class="active"
         href="/blog">blog</a>
      </li>
    
      <li>
        <a  href="/code">code</a>
      </li>
    
      <li>
        <a  href="https://drive.google.com/file/d/1cGihNgZQo_pdVjigf7tJ6LxPu7Y94_ru/view?usp=share_link">resume</a>
      </li>
    
  </ul>
</nav>


  <main>
    
    

    <div class="flex-wrapper">
      <div class="post__container">
        <div class="post">
          <header class="post__header">
            <h1 id="post__title">Graph Diffusion</h1>
            <time datetime="2022-07-25 00:00:00 &#43;0000 &#43;0000" class="post__date">Jul 25 2022</time> 
          </header>
          <article class="post__content">
              

<p>This is taken from <a href="https://arxiv.org/pdf/1511.02136v6.pdf" 
  
   target="_blank" rel="noreferrer noopener" 
>Diffusion Convolutional Neural Networks</a> (referenced in the footer). According to the authors, a <strong>diffusion convolution</strong> scans a diffusion process across each node. Analog to biology, where the information is allowed to propagate conditional of its density and environment.</p>
<p>It was applied to node classification, edge classification and graph classification, but node-classification is the task I wanted to focus on. When first presented, it was a novel way to effectively apply convolutions (invariant to location and rotation), to arbitrarily-structured data (i.e. graphs). Based on the results presented, the DCNN model outperformed a probabilistic-relational model in citation (a <em>conditional-random field</em>, no I do not know much about that) in graph topic-classification.</p>
<p>Diffusion outperforms probabilistic relational methods, is flexible to handle graphs with node features, edge features and purely structural information, is polynomial-time, model only has $H\times F$ parameters (H is num hops, F is num features), completed with a dense layer connecting $Z$ to the conditional prediction</p>
<p>Main point in the paper was using a novel way to propagate the features throughout the graph structure</p>
<pre><code class="language-python">import networkx as nx
import tensorflow as tf
import numpy as np
</code></pre>
<h1 id="simple-example">Simple Example</h1>
<p>This is getting more intimate with Graph Diffusion, since the term seemed a bit hand-wavy (especially as the authors define it by quoting as a &ldquo;diffusion mechanism&rdquo;):</p>
<blockquote>
<p><em>&ldquo;Briefly, rather than scanning a ‘square’ of parameters across a grid-structured input like the standard convolution operation, the diffusion-convolution operation builds a latent representation by scanning a diffusion process across each node in a graph-structured input&rdquo;</em></p>
</blockquote>
<pre><code class="language-python"># define a simple adjacency matrix
A = np.matrix('''
    0 1 0 1 0;
    1 0 0 1 1;
    0 0 0 1 0;
    1 1 1 0 0;
    0 1 0 0 0
''')

# feature matrix
X = np.matrix('''
    2 0;
    0 4;
    3 1;
    5 6;
    9 3
''')


# labels 
Y = np.matrix('''
    0 1;
    1 0; 
    1 0;
    1 0;
    0 1
''')

print(f'{A.shape=}, {X.shape=}, {Y.shape=}')

# just for plotting
G = nx.from_numpy_matrix(A)
nx.draw_networkx(G, with_labels=True)
</code></pre>
<pre><code>A.shape=(5, 5), X.shape=(5, 2), Y.shape=(5, 2)
</code></pre>
<img src='./graph-diffusion_files/graph-diffusion_4_1.png' />
<p>The diffusion kernel is given by:
$$
[A^{0}, A^{1},&hellip;,A^{k-1}]
$$</p>
<p>where $A$ is the adjacency matrix, and $k$ is the number of 1-hops. Normalizing the adjacency matrix is achived by dividing by the degree matrix $D$ at each stage, so it ends up being:</p>
<p>$$
A^{i+1} = \frac{A}{d}\times A^{i}
$$</p>
<p>Experimenting with $k=2$ hops, so the first dimension of $K$ would be $3$ if self-loops are included.</p>
<pre><code class="language-python">k = 2 # num-hops
K = [np.identity(A.shape[0])]

d = A.sum(0)

K.append(A / (d+1.0))
for i in range(2, k+1):
    K.append(
        np.dot(A/(d+1.0), K[-1])
    )

K = np.array(K, dtype=np.float32) # at this point, shape is [k+1, num_nodes, num_nodes]
K = np.transpose(K, (1, 0, 2)) # here it becomes (num_nodes, k+1, num_nodes)


K.shape
</code></pre>
<pre><code>(5, 3, 5)
</code></pre>
<p>Indexes diffusion kernel for node one, here&rsquo;s how to read it:</p>
<p>$i^{th}$ index is number of hops<br>
$j^{th}$ index is to which node</p>
<p>So the probability of jumping to node 1 in 1 hop 0.25 (zero-indexed), whilst probabilities for all nodes in $0$ hops are $0$ for obvious reasons</p>
<pre><code class="language-python">K[0] 
</code></pre>
<pre><code>array([[1.        , 0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.25      , 0.        , 0.25      , 0.        ],
       [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
      dtype=float32)
</code></pre>
<p>Once the diffusion kernel is defined, the next step was to weight the product of the diffusion kernel with node features using a weight matrix:</p>
<p>$$
Z = f(W^c\odot P^*X)
$$</p>
<p>$W$ is a weight matrix, let $P^*X$ be $PX$<br>
The product of the following is indexed as follows:</p>
<ul>
<li>$i^{th}$ indexes an individual node</li>
<li>$j^{th}$ is number of hops</li>
<li>$k^{th}$ is feature (I think)</li>
</ul>
<p>PX captures the probability of hopping, hence allows the features to propagate conditional to their $k$-hop distance to neighbours</p>
<pre><code class="language-python">K, X # K is kernel, X is features
</code></pre>
<pre><code>(array([[[1.        , 0.        , 0.        , 0.        , 0.        ],
         [0.        , 0.25      , 0.        , 0.25      , 0.        ],
         [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
 
        [[0.        , 1.        , 0.        , 0.        , 0.        ],
         [0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
         [0.08333334, 0.27083334, 0.125     , 0.08333334, 0.        ]],
 
        [[0.        , 0.        , 1.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.25      , 0.        ],
         [0.08333334, 0.0625    , 0.125     , 0.        , 0.        ]],
 
        [[0.        , 0.        , 0.        , 1.        , 0.        ],
         [0.33333334, 0.25      , 0.5       , 0.        , 0.        ],
         [0.08333334, 0.08333334, 0.        , 0.27083334, 0.125     ]],
 
        [[0.        , 0.        , 0.        , 0.        , 1.        ],
         [0.        , 0.25      , 0.        , 0.        , 0.        ],
         [0.08333334, 0.        , 0.        , 0.0625    , 0.125     ]]],
       dtype=float32),
 matrix([[2, 0],
         [0, 4],
         [3, 1],
         [5, 6],
         [9, 3]]))
</code></pre>
<pre><code class="language-python">PX = np.einsum('ijk,kl-&gt;ijl', K, X) # same as tf.matmul(K, X)
PX, PX.shape
</code></pre>
<pre><code>(array([[[2.        , 0.        ],
         [1.25      , 2.5       ],
         [2.14583334, 1.125     ]],
 
        [[0.        , 4.        ],
         [6.41666669, 3.        ],
         [0.95833335, 1.70833339]],
 
        [[3.        , 1.        ],
         [1.25      , 1.5       ],
         [0.54166667, 0.375     ]],
 
        [[5.        , 6.        ],
         [2.16666669, 1.5       ],
         [2.64583339, 2.3333334 ]],
 
        [[9.        , 3.        ],
         [0.        , 1.        ],
         [1.60416667, 0.75      ]]]),
 (5, 3, 2))
</code></pre>
<pre><code class="language-python">PX[1]
</code></pre>
<pre><code>array([[0.        , 4.        ],
       [6.41666669, 3.        ],
       [0.95833335, 1.70833339]])
</code></pre>
<p>$K$ =</p>
<pre><code>        [[[1.        , 0.        , 0.        , 0.        , 0.        ],
Hop 1 -&gt; [0.        , 0.25      , 0.        , 0.25      , 0.        ],
         [0.16666667, 0.0625    , 0.125     , 0.0625    , 0.125     ]],
 
        [[0.        , 1.        , 0.        , 0.        , 0.        ],
         [0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
         [0.08333334, 0.27083334, 0.125     , 0.08333334, 0.        ]],
 
        [[0.        , 0.        , 1.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        , 0.25      , 0.        ],
         [0.08333334, 0.0625    , 0.125     , 0.        , 0.        ]],
 
        [[0.        , 0.        , 0.        , 1.        , 0.        ],
         [0.33333334, 0.25      , 0.5       , 0.        , 0.        ],
         [0.08333334, 0.08333334, 0.        , 0.27083334, 0.125     ]],
 
        [[0.        , 0.        , 0.        , 0.        , 1.        ],
         [0.        , 0.25      , 0.        , 0.        , 0.        ],
         [0.08333334, 0.        , 0.        , 0.0625    , 0.125     ]]]
</code></pre>
<p>$X$ =</p>
<pre><code>        [[2, 0],
        [0, 4],
        [3, 1],
        [5, 6],
        [9, 3]]
</code></pre>
<p>For node $0$, hop $1$, feature $1$, we take $K[0, 1]$ times $X[0, 1]$:</p>
<p>This is less-optimized, but more index-friendly of rewriting the equation prior
$$
Z_ {ijk} = f\left(W^{c}_ {jk}\cdot\sum_{l=1}^{N} P_{ijl}X_{lk}\right)
$$</p>
<p>$N$ is number of nodes</p>
<p>trying $(i=1, j=1, k=0)$, which should result in $PX[1, 1, 0]$</p>
<pre><code class="language-python">K[1, 1, :], X[:, 0]
</code></pre>
<pre><code>(array([0.33333334, 0.        , 0.        , 0.25      , 0.5       ],
       dtype=float32),
 matrix([[0],
         [4],
         [1],
         [6],
         [3]]))
</code></pre>
<pre><code class="language-python">K[0, 1, 1] * X[:, 1]
</code></pre>
<pre><code>matrix([[0.  ],
        [1.  ],
        [0.25],
        [1.5 ],
        [0.75]])
</code></pre>
<pre><code class="language-python">np.sum(K[1, 1, :] * X[:, 0]) # It works! It's equal to PX[1, 1 ,0]
</code></pre>
<pre><code>6.416666686534882
</code></pre>
<pre><code class="language-python">PX[1,1,0]
</code></pre>
<pre><code>6.416666686534882
</code></pre>
<p>Past this point, $Z$ is weighted once again using another set of weights and then subject to a nonlinear activation function, standard layers implemented by <code>keras</code> layers and hence less intellectually stimulating at first glance. I may return to this paper from a purely philosophical standpoint, but the previous notes cover the parts of Tensor calculus that confused me the most.</p>
<p>For interested readers, I suggest the paper linked</p>
<h2 id="references">References<a class="anchor" href="#references">#</a></h2>
<p><a href="https://arxiv.org/pdf/1511.02136v6.pdf" 
  
   target="_blank" rel="noreferrer noopener" 
>Atwood, J., &amp; Towsley, D. (2016). Diffusion-convolutional neural networks. Advances in neural information processing systems, 29</a></p>


              
                  

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
            showMathMenu: false, //disables context menu
            tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
           }
    });
</script>
              
          </article>
          

 <div class="pagination">
  
    <a class="pagination__item" href="https://aadi-blogs.web.app/blog/tf-einsum/">
        <span class="pagination__label">Previous Post</span>
        <span class="pagination__title">Unravelling `tf.einsum`</span>
    </a>
  

  
    <a class="pagination__item" href="https://aadi-blogs.web.app/blog/awk_pad/">
      <span class="pagination__label">Next Post</span>
      <span class="pagination__title" >Zero-Padding a CSV with AWK</span>
    </a>
  
</div>

          
          <footer class="post__footer">
            


<div class="social-icons">
  
     
    
      <a
        class="social-icons__link"
        title="Twitter"
        href="https://twitter.com/tikamasaala"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/twitter.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="GitHub"
        href="https://github.com/aadi350"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/github.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="LinkedIn"
        href="https://www.linkedin.com/in/aadidev-sooknanan/"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/linkedin.svg')"></div>
      </a>
    
  
     
    
      <a
        class="social-icons__link"
        title="Email"
        href="mailto:aadidevsooknanan@gmail.com"
        target="_blank"
        rel="me noopener"
      >
        <div class="social-icons__icon" style="background-image: url('https://aadi-blogs.web.app/svg/email.svg')"></div>
      </a>
    
     
</div>

            <p>©2022 2023</p>
          </footer>
          </div>
      </div>
      
    </div>
    

  </main>

   

  
  <script src="/js/index.min.301a8b0870381bf76b3b5182e8966d363a0474281183439beb024d8b8228fc66.js" integrity="sha256-MBqLCHA4G/drO1GC6JZtNjoEdCgRg0Ob6wJNi4Io/GY=" crossorigin="anonymous"></script>
  
  
  <script src="https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js"></script>

  
  <script src="https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js"
    data-autoloader-path="https://unpkg.com/prismjs@1.20.0/components/"></script>

  


</body>

</html>
