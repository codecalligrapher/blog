<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Aadis Blog</title>
    <link>https://aadi-blogs.web.app/</link>
    <description>Recent content on Aadis Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Jul 2022 06:38:42 -0400</lastBuildDate><atom:link href="https://aadi-blogs.web.app/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PySpark Fill Rates</title>
      <link>https://aadi-blogs.web.app/code/pyspark-fill-rate/</link>
      <pubDate>Thu, 21 Jul 2022 06:38:42 -0400</pubDate>
      
      <guid>https://aadi-blogs.web.app/code/pyspark-fill-rate/</guid>
      <description>import pyspark # missing values per column for k, v in sorted( table.agg(*[ (1 - (F.count(c) / F.count(&amp;#39;*&amp;#39;))) .alias(c + &amp;#39;_miss&amp;#39;) for c in table.columns]) .collect()[0] # retrieve all data to driver .asDict() # convert to python dictionary .items() # retrieve key-value pairs , key=lambda el: el[1] , reverse=True): print(k, v) </description>
    </item>
    
    <item>
      <title>Unravelling `tf.einsum`</title>
      <link>https://aadi-blogs.web.app/blog/tf-einsum/</link>
      <pubDate>Mon, 18 Jul 2022 23:00:42 -0400</pubDate>
      
      <guid>https://aadi-blogs.web.app/blog/tf-einsum/</guid>
      <description>Origin Story Recently, I was trying to disect the original DCNN Paper which utilized a diffusion kernel to more readily make use of implicit graph-structure in common tasks such as node, edge and graph classification. However, an existing implementation I fonund had a curious piece of notation which led me down the rabbithole of Tensor calculus.
Coordinates are maps used to solve a given problem. A coordinate transform allows mapping from one frame of reference to another (converting from a map of your high school, to the location of your high school in reference to where it is in the city, compared to a country-wide map).</description>
    </item>
    
    <item>
      <title>Basics of The Adjacency Matrix</title>
      <link>https://aadi-blogs.web.app/blog/matrix-graph/</link>
      <pubDate>Wed, 13 Jul 2022 06:20:42 -0400</pubDate>
      
      <guid>https://aadi-blogs.web.app/blog/matrix-graph/</guid>
      <description>This summarizes my initial set of basic notes surrounding the adjacency matrix representation of a graph
There are multiple ways of representing graph-structured data. One of the most common ways is using the adjacency matrix, where connections between nodes are represented in a row-column format.
For example:
$$ A = \begin{bmatrix} 0 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 1 \\ 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix} $$</description>
    </item>
    
    <item>
      <title>The Graph Neural Network</title>
      <link>https://aadi-blogs.web.app/blog/graph-neural-network/</link>
      <pubDate>Sat, 25 Jun 2022 06:38:42 -0400</pubDate>
      
      <guid>https://aadi-blogs.web.app/blog/graph-neural-network/</guid>
      <description>The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&amp;rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.</description>
    </item>
    
  </channel>
</rss>
