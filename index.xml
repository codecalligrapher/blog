<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Aadis Blog</title><link>https://aadi-blogs.web.app/</link><description>Recent content on Aadis Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 18 Jul 2022 23:00:42 -0400</lastBuildDate><atom:link href="https://aadi-blogs.web.app/index.xml" rel="self" type="application/rss+xml"/><item><title>Unravelling `tf.einsum`</title><link>https://aadi-blogs.web.app/blog/tf-einsum/</link><pubDate>Mon, 18 Jul 2022 23:00:42 -0400</pubDate><guid>https://aadi-blogs.web.app/blog/tf-einsum/</guid><description>Origin Story Recently, I was trying to disect the original DCNN Paper which utilized a diffusion kernel to more readily make use of implicit graph-structure in common tasks such as node, edge and graph classification. However, an existing implementation I fonund had a curious piece of notation which led me down the rabbithole of Tensor calculus.
Coordinates are maps used to solve a given problem. A coordinate transform allows mapping from one frame of reference to another (converting from a map of your high school, to the location of your high school in reference to where it is in the city, compared to a country-wide map).</description></item><item><title>Basics of The Adjacency Matrix</title><link>https://aadi-blogs.web.app/blog/matrix-graph/</link><pubDate>Wed, 13 Jul 2022 06:20:42 -0400</pubDate><guid>https://aadi-blogs.web.app/blog/matrix-graph/</guid><description>This summarizes my initial set of basic notes surrounding the adjacency matrix representation of a graph
There are multiple ways of representing graph-structured data. One of the most common ways is using the adjacency matrix, where connections between nodes are represented in a row-column format.
For example:
$$ A = \begin{bmatrix} 0 &amp;amp; 1 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 1 \\ 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix} $$</description></item><item><title>PySpark Fill Rates</title><link>https://aadi-blogs.web.app/code/pyspark-fill-rate/</link><pubDate>Sat, 25 Jun 2022 06:38:42 -0400</pubDate><guid>https://aadi-blogs.web.app/code/pyspark-fill-rate/</guid><description>import pyspark # missing values per column for k, v in sorted( table.agg(*[ (1 - (F.count(c) / F.count(&amp;#39;*&amp;#39;))) .alias(c + &amp;#39;_miss&amp;#39;) for c in table.columns]) .collect()[0] # retrieve all data to driver .asDict() # convert to python dictionary .items() # retrieve key-value pairs , key=lambda el: el[1] , reverse=True): print(k, v)</description></item><item><title>The Graph Neural Network</title><link>https://aadi-blogs.web.app/blog/graph-neural-network/</link><pubDate>Sat, 25 Jun 2022 06:38:42 -0400</pubDate><guid>https://aadi-blogs.web.app/blog/graph-neural-network/</guid><description>The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&amp;rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.</description></item></channel></rss>