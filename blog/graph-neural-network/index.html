<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Graph Neural Network | Aadis Blog</title><meta name=keywords content><meta name=description content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings."><meta name=author content><link rel=canonical href=https://aadi-blogs.web.app/blog/graph-neural-network/><link crossorigin=anonymous href=/assets/css/stylesheet.d7fb4cbf980fe688a21621b06a795933c4e6bb2d4070ec940667af1715d84af2.css integrity="sha256-1/tMv5gP5oiiFiGwanlZM8Tmuy1AcOyUBmevFxXYSvI=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://aadi-blogs.web.app/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://aadi-blogs.web.app/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aadi-blogs.web.app/favicon-32x32.png><link rel=apple-touch-icon href=https://aadi-blogs.web.app/apple-touch-icon.png><link rel=mask-icon href=https://aadi-blogs.web.app/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="The Graph Neural Network"><meta property="og:description" content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings."><meta property="og:type" content="article"><meta property="og:url" content="https://aadi-blogs.web.app/blog/graph-neural-network/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-06-25T06:38:42-04:00"><meta property="article:modified_time" content="2022-06-25T06:38:42-04:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Graph Neural Network"><meta name=twitter:description content="The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.
(If you need a refresher on deep learning, see here)
The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings."><meta name=twitter:site content="@https://twitter.com/cats"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"https://aadi-blogs.web.app/blog/"},{"@type":"ListItem","position":2,"name":"The Graph Neural Network","item":"https://aadi-blogs.web.app/blog/graph-neural-network/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Graph Neural Network","name":"The Graph Neural Network","description":"The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.\n(If you need a refresher on deep learning, see here)\nThe idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node\u0026rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.","keywords":[],"articleBody":" The Graph Neural Network (GNN) was proposed (Scarselli, 2008) as a general framework for defining deep neural networks on graph data.\n(If you need a refresher on deep learning, see here)\nThe idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node’s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.\nThis task is further complicated by the fact that typical deep-learning approaches (Convolutional and Recurrent Neural Networks) expect some form of data structured in the Euclidean plane (images or sequences of text). Hence, a completely new way of utilizing deep, multi-layer perceptrons was needed.\nBefore we go further, here are two concepts that are fairly significant to the field: Inductive and Transductive Learning\nInductive learning is what you’d think about as typical, supervised machine learning. This is where a model learns general rules from observed training data, which are then applied to test cases which are unseen during training. Although the model is exposed to a restricted scope of training data, it is expected to generalize by learning latent pattern present in a feature-target relationship.\nTranductive learning uses both the training and testing data during the learning phase. In this case, the model is, for example, aware of test-node in a graph, but attempts to find information in the combined dataset for later use in predicting the unlabelled data points\nNow that we’ve gotten definitions out of the way, we need to define some method, or set of functions by which our deep embeddings can be generated. Additionally, these embeddings need be permutation invariant and equivariant. (This is why we can’t simply feed the adjacency matrix into a neural network; The order of the nodes in the matrix would impact the actual solutions approximated by the network, and the number of parameters in the network would severely outstrip the number of nodes thereby inducing inherent instability in training and result in overfitting)\nMathematically, if $\\bold{P}$ is the permutation matrix:\n$$ f(\\textbf{PAP}^T) = f(\\textbf{A}) $$ $$ f(\\textbf{PAP}^T) = \\textbf{P}f(\\textbf{A}) $$\nPermutation invariance means that the output doesn’t depend on how the rows and columns are ordered (multiple adjacency matrices can represent the same graph). Permutation equivariance means that the output of $f$ is permuted in a consistent way when $\\bold{A}$ is permuted.\nThe Basic Neural Network Model I won’t go through the motivations of how the GNN materialized, but I can think of it as a generalization of convolutions to non-Euclidean data. The idea is to have a vector $\\bold{h}_u$, or hidden-state for each node $u$, which is updated iteratively by gaining information from its neighbourhood. (This is known as neural message passing, because the updates from the neighbourhood is received via a nonlinear activation function wrapping a neural network)\nThe way we can mathematically generalize this notion is by the following:\nWe generate a node embedding $\\bold{h}_u^{(k+1)}$ (for node $u$ from its $k+1$-hop neighbourhood), by applying some function $U$, to both its own embedding from the previous ($k^{th}$) iteration, as well as from the aggregate of its neighbours’ embeddings. Less confusingly:\n$$ \\bold{h}_u^{k+1} = U^k(\\bold{h}_u^k, AGG^k({\\bold{h}_v^k)), \\forall v\\in\\mathcal{N}_u} $$\nwhere $\\mathcal{N}_u$ are all the nodes in the neighbourhood of node $u$. To put it more simply, we can think of the aggregate information as a neural “message” from the node neighbours, passed through the update function, along with the node’s previous state at the previous iteration.\nAt each iteration, a node learns more about its wider neigbourhood, and as such, the above is not iterated to convergence, but is instead iterated for a fixed, pre-determined $K$ set of times. More concretely, at iteration $k=1$, a node has information from its immediate, one-hop neighbourhood (nodes that can be reached using a path length of one in the graph). In general, after $k$ iterations, every node contains information about its $k$-hop neighbourhood.\nThis information is composed both structural and feature-based components. Structure would comprise encoded information about the density and connectivity of the node’s neighbourhood, while feature-based information aggregation would be analagous to the operation of convolutional kernels in a pixel-neighbourhood.\nThinking About Implementation In order to concretize the above formulation, we need to define actual functions for the update and aggregate step. As given in the 2008 paper, the aggregate function is given by:\n$$ \\boldsymbol{m}_{N(u)} = \\sum_u\\boldsymbol{h}_v, \\forall u \\in \\mathcal{N}_u $$\nand the update function is given by:\n$$ U = \\sigma\\left(\\boldsymbol{W}_uh_u+\\boldsymbol{W}_n\\boldsymbol{m}_n\\right) $$\nTypes of Aggregate Functions Taking the sum of node features is highly sensitive to the number of nodes in a given neighbourhood, as such different ways of normalizing the aggregate function have been proposed:\n1. Neighbourhood Normalization A simple way to account for varying ranges of node degrees is to simply normalize the sum of node features by the number of nodes. $$ \\boldsymbol{M}_\\mathcal{N(u)}=\\frac{\\sum_v \\boldsymbol{h}_v}{|\\mathcal{N}(u)|} $$\nOthers, such as the symmetric normalization used for citation networks, idea being that high-degree nodes may not be useful for finding communities, since they are cited across many diverse subfields. $$ \\boldsymbol{M}_\\mathcal{N(u)}=\\sum_v\\frac{\\boldsymbol{h}_v}{\\sqrt{|\\mathcal{N(u)}||\\mathcal{N(v)}|}} $$\n2. Graph Convolutions This was proposed in this paper in 2015 and is based on the Fourier decomposition of graph-signals. The idea is that eigenvectors of the graph Laplacian is associated with a corresponding eigenvalue analagous to the complex exponential at a certain frequency. The message-passing function is therefore defined as: $$ \\boldsymbol{h}_u^k=,\\sigma\\left(\\boldsymbol{W}\\sum_v\\frac{\\boldsymbol{h}_v}{\\sqrt{|\\mathcal{N(u)}||\\mathcal{N(v)}|}}\\right) \\in\\mathcal{N}(u)\\cup {u} $$ It is of note that using the above formulation, we also use the concept of self-loops, in order to eliminate an explicit update step, where aggregation is taken over the joing set $\\mathcal{N}\\cup {u}$\n3. Neighbourhood Pooling Any permutation-invariant function which maps to a single embedding is suitable for the aggregate function. One way to do this is by using an arbitrarily deep multi-layer perceptron MLP with some trainable parameters $t$. For example, using a sum function: $$ \\boldsymbol{m}_{\\mathcal{N}(u)}=\\text{MLP}_t \\left(\\sum_v \\text{MLP}_p (\\boldsymbol{h}_v)\\right) $$\n4. Attention This is possibly the most hyped topic in machine learning over the past 5 years since the 2015 paper and the subsequent explosion of multi-head attention (known as transformers). The fundamental predicate is in weighting each neighbour based on their influence (left up to interpretation) during the aggregation step. For example:\n$$ \\bold{m}_{\\mathcal{N(u)}}=\\sum_v \\alpha _{u,v}h_v, v\\in\\mathcal{N(u)} $$ You can see here for the original Graph-attention paper to see how the attention mechanism was defined.\nGAT is shown to work well with graph data, where others have applied differing ways of aggregating the attention mechanisms\n# basic import dgl import dgl.function as fn import torch import torch.nn as nn from torch.nn import init import torch.nn.functional as F class GraphConv(nn.Module): def __init__(self, in_feat, out_feat, k=1): super(GraphConv, self).__init__() self.fc = nn.Linear(in_feat, out_feat, bias=True) self.k = k nn.init.xavier_uniform_(self.fc.weight) nn.init.zeros_(self.fc.bias) def forward(self, graph, feat): msg_func = fn.copy_u(\"h\", \"m\") degs = graph.in_degrees().float().clamp(min=1) norm = torch.pow(degs, -0.5) norm = norm.to(feat.device).unsqueeze(1) # hop-step for _ in range(self.k): graph.ndata['h'] = feat graph.update_all(msg_func, fn.sum('m', 'h')) feat = graph.ndata.pop('h') feat = feat * norm return self.fc(feat) # GraphSAGE import dgl.function as fn class SAGEConv(nn.Module): \"\"\"Graph convolution module used by the GraphSAGE model. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. \"\"\" def __init__(self, in_feat, out_feat): super(SAGEConv, self).__init__() # A linear submodule for projecting the input and neighbor feature to the output. self.linear = nn.Linear(in_feat * 2, out_feat) def forward(self, g, h): \"\"\"Forward computation Parameters ---------- g : Graph The input graph. h : Tensor The input node feature. \"\"\" with g.local_scope(): g.ndata['h'] = h # update_all is a message passing API. g.update_all(message_func=fn.copy_u('h', 'm'), reduce_func=fn.mean('m', 'h_N')) h_N = g.ndata['h_N'] h_total = torch.cat([h, h_N], dim=1) return self.linear(h_total) Types of Update Functions While the aggregate function defines how the data is treated as it arrives at each node, the update function defines where the data moves between nodes. A common issue is over-smoothing, making it impossible to build deeper models. This is due to the fact that the common message-passing paradigm is essentially a low-pass filter over the graph. As the signal is propagated throughout the graph, the high-frequency patterns get lost in the constancy of the low frequencies. This results in the updated node representations which depend too strongly on incoming message from neighbours, at expense of node embeddings from previous neighbours.\nTwo ways of addressing this are: skip connections and gated updates.\n1. Skip Connections This is analagous to drop-out and skip-connections from computer vision. Essentially, only updates from certain nodes are allowed to arrive at a given node. This is implemented by concatenating the output of the update function with the node’s previous-layer representation (like GraphSAGE). Another interpretation is by linearly interpolating between the current and new node values to achieve updated state:\n$$ \\text{UPDATE} _\\text{interpolate}(\\boldsymbol{h}_u, \\boldsymbol{m} _\\mathcal{N(u)})=\\alpha\\circ\\text{UPDATE}(\\boldsymbol{h}_u, \\boldsymbol{m} _\\mathcal{N(u)})+\\alpha_2\\circ\\boldsymbol{h}_u $$\n$\\alpha_1, \\alpha_2\\in [0,1]^d$ are gating vectors s.t. $\\alpha_2 = 1-\\alpha_1$. Updated representation is a linear interpolation between previous embedding and (vanilla) updated embedding.\nSkip-connections address over-smoothing and numerical stability during optimization.\n2. Gated Updates If skip-connections are analagous to dropout in Convolutional Neural Networks, gated updates are analagous to the Gated-Recurrent Unit in the RNN world. Here, an aggregation function receives an observation from its neighbours, which is then used to update a hidden node state. In this case, we can apply basic RNN/GRU logic: $$ \\bold{h} _u^{(k)}=\\text{GRU}(\\bold{h} _u^{k-1}, \\bold{m} _{\\mathcal{N}(u)}^k) $$\nclass GatedGraphConv(nn.Module): def __init__(self, in_feats, out_feats, n_steps, n_etypes, bias=True): super(GatedGraphConv, self).__init__() self.in_feats = in_feats self.out_feats = out_feats self.n_steps = n_steps self.n_etypes = n_etypes self.linear_layers = nn.ModuleList( [nn.Linear(out_feats, out_feats) for _ in range(n_etypes)] ) self.gru = nn.GRUCell(out_feats, out_feats, bias=bias) gain = init.calculate_gain('relu') self.gru.reset_parameters() for linear in self.linear_layers: init.xavier_normal_(linear.weight, gain=gain) init.zeros_(linear.bias) def forward(self, graph, feat, etypes=None): with graph.local_scope(): zero_pad = feat.new_zeros( (feat.shape[0], self.out_feats - feat.shape[1]) ) feat = torch.cat([feat, zero_pad],-1) for _ in range(self.steps): graph.ndata['h'] = feat for i in range(self.n_etypes): eids = torch.nonzero( etypes==i, as_tuple=False ).view(-1).type(graph.idtype) if len(eids) \u003e 0: graph.apply_edges( lambda edges: { 'W_e*h': self.linear_layers[i](edges.src['h']) } ) graph.update_all(fn.copy_e('W_e*h', 'm'), fn.sum('m', 'a')) a = graph.ndata.pop('a') feat = self.gru(a, feat) return feat g=GatedGraphConv(10,2, 2,3) Notes on MPNNs The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a node’s neighborhood into a dense vector embedding. These node embeddings can then be fed to downstream machine learning systems and aid in tasks such as node classification, clustering, and link prediction. MPNNs can additionally generalize to much larger graphs (see here)\nMPNNs Limits Message-passing has linear time complexity (see Breaking the Limits of Message Passing Graph Neural Networks). This may be a limit depending on what architecture it is compared to. (For example, even basic CNNs usually are not linear).\nIt it theoretically impractical to make an MPNN more powerful in terms of the 1-WL test. The 1-WL test is a standard measure of the ability of a particular model to differentiate between non-isomorphic graphs. (Graphs are isomorphic if a relabelling of one graph results in another).\n1-WL graphs (MPNNs) cannot count the number of cycles, triangles and other strucutral features, informative for some social and/or chemical graphs (see here).\nHowever, heres is an interesting paper that practically superceeds the expressive power of the 1-WL test (http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf). Additionally, more “features” of the graph have been proposed to potentially increase its ability in terms of 1-WL, such as adding trainable weights for:\nDistance between nodes (Deferard 2016) Connected node features GAT Edge-features (Bresson and Laurent 2018) ","wordCount":"1898","inLanguage":"en","datePublished":"2022-06-25T06:38:42-04:00","dateModified":"2022-06-25T06:38:42-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://aadi-blogs.web.app/blog/graph-neural-network/"},"publisher":{"@type":"Organization","name":"Aadis Blog","logo":{"@type":"ImageObject","url":"https://aadi-blogs.web.app/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aadi-blogs.web.app/ accesskey=h title="Aadis Blog (Alt + H)">Aadis Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aadi-blogs.web.app/blog/ title=blogs><span><i class='fa fa-heart'></i>blogs</span></a></li><li><a href=https://aadi-blogs.web.app/code/ title=code><span><i class='fa fa-heart'></i>code</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aadi-blogs.web.app/>Home</a>&nbsp;»&nbsp;<a href=https://aadi-blogs.web.app/blog/>Blogs</a></div><h1 class=post-title>The Graph Neural Network</h1><div class=post-meta><span title='2022-06-25 06:38:42 -0400 -0400'>June 25, 2022</span>&nbsp;·&nbsp;9 min</div></header><div class=post-content><p>The Graph Neural Network (GNN) was proposed (<a href="https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers">Scarselli, 2008</a>) as a general framework for defining deep neural networks on graph data.</p><p>(If you need a refresher on deep learning, see <a href="https://www.youtube.com/watch?v=tutlI9YzJ2g">here</a>)</p><p>The idea was to somehow utilize a deep neural network to generate node-embeddings in a generalize-able way to graph-structured data. The main idea in utilizing neural networks was that, apart from node features (degree, attributes, etc), the actual structure of the node&rsquo;s neighbourhood, and by extension the graph, should contribute somehow to the node embeddings.</p><p>This task is further complicated by the fact that typical deep-learning approaches (Convolutional and Recurrent Neural Networks) expect some form of data structured in the Euclidean plane (images or sequences of text). Hence, a completely new way of utilizing deep, multi-layer perceptrons was needed.</p><p>Before we go further, here are two concepts that are fairly significant to the field: Inductive and Transductive Learning</p><p><strong>Inductive learning</strong> is what you&rsquo;d think about as typical, supervised machine learning. This is where a model learns general rules from observed training data, which are then applied to test cases which are unseen during training. Although the model is exposed to a restricted scope of training data, it is expected to generalize by learning latent pattern present in a feature-target relationship.</p><p><strong>Tranductive learning</strong> uses both the training and testing data during the learning phase. In this case, the model is, for example, aware of test-node in a graph, but attempts to find information in the combined dataset for later use in predicting the unlabelled data points</p><p>Now that we&rsquo;ve gotten definitions out of the way, we need to define some method, or set of functions by which our deep embeddings can be generated. Additionally, these embeddings need be permutation invariant and equivariant. (This is why we can&rsquo;t simply feed the adjacency matrix into a neural network; The order of the nodes in the matrix would impact the actual solutions approximated by the network, and the number of parameters in the network would severely outstrip the number of nodes thereby inducing inherent instability in training and result in overfitting)</p><p>Mathematically, if $\bold{P}$ is the permutation matrix:</p><p>$$
f(\textbf{PAP}^T) = f(\textbf{A})
$$
$$
f(\textbf{PAP}^T) = \textbf{P}f(\textbf{A})
$$</p><p>Permutation invariance means that the output doesn&rsquo;t depend on how the rows and columns are ordered (multiple adjacency matrices can represent the same graph). Permutation equivariance means that the output of $f$ is permuted in a consistent way when $\bold{A}$ is permuted.</p><h2 id=the-basic-neural-network-model>The Basic Neural Network Model<a hidden class=anchor aria-hidden=true href=#the-basic-neural-network-model>#</a></h2><p>I won&rsquo;t go through the motivations of how the GNN materialized, but I can think of it as a generalization of convolutions to non-Euclidean data. The idea is to have a vector $\bold{h}_u$, or hidden-state for each node $u$, which is updated iteratively by gaining information from its neighbourhood. (This is known as <em>neural message passing</em>, because the updates from the neighbourhood is received via a nonlinear activation function wrapping a neural network)</p><p>The way we can mathematically generalize this notion is by the following:</p><p>We generate a node embedding $\bold{h}_u^{(k+1)}$ (for node $u$ from its $k+1$-hop neighbourhood), by applying some function $U$, to both its own embedding from the previous ($k^{th}$) iteration, as well as from the aggregate of its neighbours&rsquo; embeddings. Less confusingly:</p><p>$$
\bold{h}_u^{k+1} = U^k(\bold{h}_u^k, AGG^k({\bold{h}_v^k)), \forall v\in\mathcal{N}_u}
$$</p><p>where $\mathcal{N}_u$ are all the nodes in the neighbourhood of node $u$. To put it more simply, we can think of the aggregate information as a neural &ldquo;message&rdquo; from the node neighbours, passed through the update function, along with the node&rsquo;s previous state at the previous iteration.</p><p>At each iteration, a node learns more about its wider neigbourhood, and as such, the above is not iterated to convergence, but is instead iterated for a fixed, pre-determined $K$ set of times. More concretely, at iteration $k=1$, a node has information from its immediate, one-hop neighbourhood (nodes that can be reached using a path length of one in the graph). In general, after $k$ iterations, every node contains information about its $k$-hop neighbourhood.</p><p>This information is composed both structural and feature-based components. Structure would comprise encoded information about the density and connectivity of the node&rsquo;s neighbourhood, while feature-based information aggregation would be analagous to the operation of convolutional kernels in a pixel-neighbourhood.</p><h1 id=thinking-about-implementation>Thinking About Implementation<a hidden class=anchor aria-hidden=true href=#thinking-about-implementation>#</a></h1><p>In order to concretize the above formulation, we need to define actual functions for the update and aggregate step. As given in the <a href=https://ieeexplore.ieee.org/abstract/document/4700287>2008 paper</a>, the aggregate function is given by:</p><p>$$
\boldsymbol{m}_{N(u)} = \sum_u\boldsymbol{h}_v, \forall u \in \mathcal{N}_u
$$</p><p>and the update function is given by:</p><p>$$
U = \sigma\left(\boldsymbol{W}_uh_u+\boldsymbol{W}_n\boldsymbol{m}_n\right)
$$</p><h2 id=types-of-aggregate-functions>Types of Aggregate Functions<a hidden class=anchor aria-hidden=true href=#types-of-aggregate-functions>#</a></h2><p>Taking the sum of node features is highly sensitive to the number of nodes in a given neighbourhood, as such different ways of normalizing the aggregate function have been proposed:</p><h3 id=1-neighbourhood-normalization>1. Neighbourhood Normalization<a hidden class=anchor aria-hidden=true href=#1-neighbourhood-normalization>#</a></h3><p>A simple way to account for varying ranges of node degrees is to simply normalize the sum of node features by the number of nodes.
$$
\boldsymbol{M}_\mathcal{N(u)}=\frac{\sum_v \boldsymbol{h}_v}{|\mathcal{N}(u)|}
$$</p><p>Others, such as the symmetric normalization used for citation networks, idea being that high-degree nodes may not be useful for finding communities, since they are cited across many diverse subfields.
$$
\boldsymbol{M}_\mathcal{N(u)}=\sum_v\frac{\boldsymbol{h}_v}{\sqrt{|\mathcal{N(u)}||\mathcal{N(v)}|}}
$$</p><h3 id=2-graph-convolutions>2. Graph Convolutions<a hidden class=anchor aria-hidden=true href=#2-graph-convolutions>#</a></h3><p>This was proposed in <a href=https://arxiv.org/abs/1609.02907>this paper</a> in 2015 and is based on the Fourier decomposition of graph-signals. The idea is that eigenvectors of the graph Laplacian is associated with a corresponding eigenvalue analagous to the complex exponential at a certain frequency. The message-passing function is therefore defined as:
$$
\boldsymbol{h}_u^k=,\sigma\left(\boldsymbol{W}\sum_v\frac{\boldsymbol{h}_v}{\sqrt{|\mathcal{N(u)}||\mathcal{N(v)}|}}\right) \in\mathcal{N}(u)\cup {u}
$$
It is of note that using the above formulation, we also use the concept of self-loops, in order to eliminate an explicit update step, where aggregation is taken over the joing set $\mathcal{N}\cup {u}$</p><h3 id=3-neighbourhood-pooling>3. Neighbourhood Pooling<a hidden class=anchor aria-hidden=true href=#3-neighbourhood-pooling>#</a></h3><p>Any permutation-invariant function which maps to a single embedding is suitable for the aggregate function. One way to do this is by using an arbitrarily deep multi-layer perceptron MLP with some trainable parameters $t$. For example, using a sum function:
$$
\boldsymbol{m}_{\mathcal{N}(u)}=\text{MLP}_t \left(\sum_v \text{MLP}_p (\boldsymbol{h}_v)\right)
$$</p><h3 id=4-attention>4. Attention<a hidden class=anchor aria-hidden=true href=#4-attention>#</a></h3><p>This is possibly the most hyped topic in machine learning over the past 5 years since the <a href=https://proceedings.neurips.cc/paper/2015/hash/1068c6e4c8051cfd4e9ea8072e3189e2-Abstract.html>2015 paper</a> and the subsequent explosion of multi-head attention (known as transformers). The fundamental predicate is in weighting each neighbour based on their influence (left up to interpretation) during the aggregation step. For example:</p><p>$$
\bold{m}_{\mathcal{N(u)}}=\sum_v \alpha _{u,v}h_v, v\in\mathcal{N(u)}
$$
You can see <a href>here</a> for the original Graph-attention paper to see how the attention mechanism was defined.</p><p>GAT is shown to work well with graph data, where others have applied differing ways of aggregating the attention mechanisms</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># basic</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dgl 
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dgl.function <span style=color:#66d9ef>as</span> fn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.nn <span style=color:#f92672>import</span> init
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F 
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GraphConv</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_feat, out_feat, k<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        super(GraphConv, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>fc <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(in_feat, out_feat, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>k <span style=color:#f92672>=</span> k
</span></span><span style=display:flex><span>        nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>xavier_uniform_(self<span style=color:#f92672>.</span>fc<span style=color:#f92672>.</span>weight)
</span></span><span style=display:flex><span>        nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>zeros_(self<span style=color:#f92672>.</span>fc<span style=color:#f92672>.</span>bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, graph, feat):
</span></span><span style=display:flex><span>        msg_func <span style=color:#f92672>=</span> fn<span style=color:#f92672>.</span>copy_u(<span style=color:#e6db74>&#34;h&#34;</span>, <span style=color:#e6db74>&#34;m&#34;</span>)
</span></span><span style=display:flex><span>        degs <span style=color:#f92672>=</span> graph<span style=color:#f92672>.</span>in_degrees()<span style=color:#f92672>.</span>float()<span style=color:#f92672>.</span>clamp(min<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        norm <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>pow(degs, <span style=color:#f92672>-</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>        norm <span style=color:#f92672>=</span> norm<span style=color:#f92672>.</span>to(feat<span style=color:#f92672>.</span>device)<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># hop-step</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>k):
</span></span><span style=display:flex><span>            graph<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#39;h&#39;</span>] <span style=color:#f92672>=</span> feat
</span></span><span style=display:flex><span>            graph<span style=color:#f92672>.</span>update_all(msg_func, fn<span style=color:#f92672>.</span>sum(<span style=color:#e6db74>&#39;m&#39;</span>, <span style=color:#e6db74>&#39;h&#39;</span>))
</span></span><span style=display:flex><span>            feat <span style=color:#f92672>=</span> graph<span style=color:#f92672>.</span>ndata<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#39;h&#39;</span>)
</span></span><span style=display:flex><span>            feat <span style=color:#f92672>=</span> feat <span style=color:#f92672>*</span> norm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>fc(feat)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># GraphSAGE</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> dgl.function <span style=color:#66d9ef>as</span> fn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SAGEConv</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Graph convolution module used by the GraphSAGE model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    ----------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    in_feat : int
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Input feature size.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    out_feat : int
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Output feature size.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_feat, out_feat):
</span></span><span style=display:flex><span>        super(SAGEConv, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        <span style=color:#75715e># A linear submodule for projecting the input and neighbor feature to the output.</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(in_feat <span style=color:#f92672>*</span> <span style=color:#ae81ff>2</span>, out_feat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, g, h):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Forward computation
</span></span></span><span style=display:flex><span><span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Parameters
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        ----------
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        g : Graph
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            The input graph.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        h : Tensor
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            The input node feature.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> g<span style=color:#f92672>.</span>local_scope():
</span></span><span style=display:flex><span>            g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#39;h&#39;</span>] <span style=color:#f92672>=</span> h
</span></span><span style=display:flex><span>            <span style=color:#75715e># update_all is a message passing API.</span>
</span></span><span style=display:flex><span>            g<span style=color:#f92672>.</span>update_all(message_func<span style=color:#f92672>=</span>fn<span style=color:#f92672>.</span>copy_u(<span style=color:#e6db74>&#39;h&#39;</span>, <span style=color:#e6db74>&#39;m&#39;</span>), reduce_func<span style=color:#f92672>=</span>fn<span style=color:#f92672>.</span>mean(<span style=color:#e6db74>&#39;m&#39;</span>, <span style=color:#e6db74>&#39;h_N&#39;</span>))
</span></span><span style=display:flex><span>            h_N <span style=color:#f92672>=</span> g<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#39;h_N&#39;</span>]
</span></span><span style=display:flex><span>            h_total <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([h, h_N], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>linear(h_total)
</span></span></code></pre></div><h1 id=types-of-update-functions>Types of Update Functions<a hidden class=anchor aria-hidden=true href=#types-of-update-functions>#</a></h1><p>While the aggregate function defines <em>how</em> the data is treated as it arrives at each node, the update function defines <em>where</em> the data moves between nodes. A common issue is over-smoothing, making it impossible to build deeper models. This is due to the fact that the common message-passing paradigm is <a href="https://openreview.net/forum?id=-qh0M9XWxnv">essentially a low-pass filter over the graph</a>. As the signal is propagated throughout the graph, the high-frequency patterns get lost in the constancy of the low frequencies. This results in the updated node representations which depend too strongly on incoming message from neighbours, at expense of node embeddings from previous neighbours.</p><p>Two ways of addressing this are: skip connections and gated updates.</p><h2 id=1-skip-connections>1. Skip Connections<a hidden class=anchor aria-hidden=true href=#1-skip-connections>#</a></h2><p>This is analagous to drop-out and skip-connections from computer vision. Essentially, only updates from certain nodes are allowed to arrive at a given node. This is implemented by concatenating the output of the update function with the node&rsquo;s previous-layer representation (like GraphSAGE). Another interpretation is by linearly interpolating between the current and new node values to achieve updated state:</p><p>$$
\text{UPDATE} _\text{interpolate}(\boldsymbol{h}_u, \boldsymbol{m} _\mathcal{N(u)})=\alpha\circ\text{UPDATE}(\boldsymbol{h}_u, \boldsymbol{m} _\mathcal{N(u)})+\alpha_2\circ\boldsymbol{h}_u
$$</p><p>$\alpha_1, \alpha_2\in [0,1]^d$ are gating vectors s.t. $\alpha_2 = 1-\alpha_1$. Updated representation is a linear interpolation between previous embedding and (vanilla) updated embedding.</p><p>Skip-connections address over-smoothing and numerical stability during optimization.</p><h2 id=2-gated-updates>2. Gated Updates<a hidden class=anchor aria-hidden=true href=#2-gated-updates>#</a></h2><p>If skip-connections are analagous to dropout in Convolutional Neural Networks, gated updates are analagous to the Gated-Recurrent Unit in the RNN world. Here, an aggregation function receives an observation from its neighbours, which is then used to update a hidden node state. In this case, we can apply basic RNN/GRU logic:
$$
\bold{h} _u^{(k)}=\text{GRU}(\bold{h} _u^{k-1}, \bold{m} _{\mathcal{N}(u)}^k)
$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>GatedGraphConv</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, in_feats, out_feats, n_steps, n_etypes, bias<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        super(GatedGraphConv, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>in_feats <span style=color:#f92672>=</span> in_feats
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>out_feats <span style=color:#f92672>=</span> out_feats
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_steps <span style=color:#f92672>=</span> n_steps
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_etypes <span style=color:#f92672>=</span> n_etypes
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>linear_layers <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList(
</span></span><span style=display:flex><span>            [nn<span style=color:#f92672>.</span>Linear(out_feats, out_feats) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_etypes)]
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gru <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GRUCell(out_feats, out_feats, bias<span style=color:#f92672>=</span>bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        gain <span style=color:#f92672>=</span> init<span style=color:#f92672>.</span>calculate_gain(<span style=color:#e6db74>&#39;relu&#39;</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gru<span style=color:#f92672>.</span>reset_parameters()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> linear <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>linear_layers:
</span></span><span style=display:flex><span>            init<span style=color:#f92672>.</span>xavier_normal_(linear<span style=color:#f92672>.</span>weight, gain<span style=color:#f92672>=</span>gain)
</span></span><span style=display:flex><span>            init<span style=color:#f92672>.</span>zeros_(linear<span style=color:#f92672>.</span>bias)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, graph, feat, etypes<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> graph<span style=color:#f92672>.</span>local_scope():
</span></span><span style=display:flex><span>            zero_pad <span style=color:#f92672>=</span> feat<span style=color:#f92672>.</span>new_zeros(
</span></span><span style=display:flex><span>                (feat<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], self<span style=color:#f92672>.</span>out_feats <span style=color:#f92672>-</span> feat<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            feat <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat([feat, zero_pad],<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>steps):
</span></span><span style=display:flex><span>                graph<span style=color:#f92672>.</span>ndata[<span style=color:#e6db74>&#39;h&#39;</span>] <span style=color:#f92672>=</span> feat
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(self<span style=color:#f92672>.</span>n_etypes):
</span></span><span style=display:flex><span>                    eids <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>nonzero(
</span></span><span style=display:flex><span>                        etypes<span style=color:#f92672>==</span>i, as_tuple<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>                    )<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>type(graph<span style=color:#f92672>.</span>idtype)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> len(eids) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        graph<span style=color:#f92672>.</span>apply_edges(
</span></span><span style=display:flex><span>                            <span style=color:#66d9ef>lambda</span> edges: {
</span></span><span style=display:flex><span>                                <span style=color:#e6db74>&#39;W_e*h&#39;</span>: self<span style=color:#f92672>.</span>linear_layers[i](edges<span style=color:#f92672>.</span>src[<span style=color:#e6db74>&#39;h&#39;</span>])
</span></span><span style=display:flex><span>                            }
</span></span><span style=display:flex><span>                        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            graph<span style=color:#f92672>.</span>update_all(fn<span style=color:#f92672>.</span>copy_e(<span style=color:#e6db74>&#39;W_e*h&#39;</span>, <span style=color:#e6db74>&#39;m&#39;</span>), fn<span style=color:#f92672>.</span>sum(<span style=color:#e6db74>&#39;m&#39;</span>, <span style=color:#e6db74>&#39;a&#39;</span>))
</span></span><span style=display:flex><span>            a <span style=color:#f92672>=</span> graph<span style=color:#f92672>.</span>ndata<span style=color:#f92672>.</span>pop(<span style=color:#e6db74>&#39;a&#39;</span>)
</span></span><span style=display:flex><span>            feat  <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gru(a, feat)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> feat
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>g<span style=color:#f92672>=</span>GatedGraphConv(<span style=color:#ae81ff>10</span>,<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>)
</span></span></code></pre></div><h2 id=notes-on-mpnns>Notes on MPNNs<a hidden class=anchor aria-hidden=true href=#notes-on-mpnns>#</a></h2><p>The basic idea behind node embedding approaches is to use dimensionality reduction techniques to distill the
high-dimensional information about a node’s neighborhood into a dense vector embedding. These
node embeddings can then be fed to downstream machine learning systems and aid in tasks such as
node classification, clustering, and link prediction. MPNNs can additionally generalize to much larger graphs (see <a href>here</a>)</p><h3 id=mpnns-limits>MPNNs Limits<a hidden class=anchor aria-hidden=true href=#mpnns-limits>#</a></h3><p>Message-passing has linear time complexity (see <a href=http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf>Breaking the Limits of Message Passing Graph Neural Networks</a>). This may be a limit depending on what architecture it is compared to. (For example, even basic CNNs usually are not linear).</p><p>It it theoretically impractical to make an MPNN more powerful in terms of the 1-WL test. The <a href=https://arxiv.org/pdf/2201.07083.pdf>1-WL test</a> is a standard measure of the ability of a particular model to differentiate between non-isomorphic graphs. (Graphs are isomorphic if a relabelling of one graph results in another).</p><p>1-WL graphs (MPNNs) cannot count the number of cycles, triangles and other strucutral features, informative for some social and/or chemical graphs (see <a href=https://arxiv.org/pdf/2201.07083.pdf>here</a>).</p><p>However, heres is an interesting paper that practically superceeds the expressive power of the 1-WL test (<a href=http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf)>http://proceedings.mlr.press/v139/balcilar21a/balcilar21a.pdf)</a>. Additionally, more &ldquo;features&rdquo; of the graph have been proposed to potentially increase its ability in terms of 1-WL, such as adding trainable weights for:</p><ul><li>Distance between nodes (Deferard 2016)</li><li>Connected node features GAT</li><li>Edge-features (Bresson and Laurent 2018)</li></ul></div><footer class=post-footer><ul class=post-tags></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on twitter" href="https://twitter.com/intent/tweet/?text=The%20Graph%20Neural%20Network&url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f&title=The%20Graph%20Neural%20Network&summary=The%20Graph%20Neural%20Network&source=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on reddit" href="https://reddit.com/submit?url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f&title=The%20Graph%20Neural%20Network"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on whatsapp" href="https://api.whatsapp.com/send?text=The%20Graph%20Neural%20Network%20-%20https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Graph Neural Network on telegram" href="https://telegram.me/share/url?text=The%20Graph%20Neural%20Network&url=https%3a%2f%2faadi-blogs.web.app%2fblog%2fgraph-neural-network%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://aadi-blogs.web.app/>Aadis Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>